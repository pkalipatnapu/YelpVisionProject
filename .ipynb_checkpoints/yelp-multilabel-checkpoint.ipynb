{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel classification on Yelp Dataset\n",
    "\n",
    "Author: prad@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a version of the caffe example notebook for the pascal multilabel classification problem. It has been adapted for the Yelp Attribute detection problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminaries\n",
    "\n",
    "* First, make sure you compile caffe using\n",
    "WITH_PYTHON_LAYER := 1\n",
    "\n",
    "* Second, download Yelp Dataset to ~/Yelp. This file should be in ~/YelpVisionProject\n",
    "\n",
    "* Third, import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name tools",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1eecc8d431da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pycaffe\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the tools file is in this folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcaffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtools\u001b[0m \u001b[1;31m#this contains some tools that we need\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name tools"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "% matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 6)\n",
    "\n",
    "caffe_root = '../caffe'  # this file is expected to be in ~/YelpProjectVision. Caffe is expected to be ~/Caffe\n",
    "sys.path.append(caffe_root + 'python')\n",
    "import caffe # If you get \"No module named _caffe\", either you have not built pycaffe or you have the wrong path.\n",
    "\n",
    "from caffe import layers as L, params as P # Shortcuts to define the net prototxt.\n",
    "\n",
    "sys.path.append(\"pycaffe/layers\") # the datalayers we will use are in this directory.\n",
    "sys.path.append(\"pycaffe\") # the tools file is in this folder\n",
    "\n",
    "import tools #this contains some tools that we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fourth, set data directories and initialize caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set data root directory, e.g:\n",
    "yelp_root = '../Yelp/data'\n",
    "\n",
    "# these are the PASCAL classes, we'll need them later.\n",
    "classes = np.asarray(['good_for_lunch', 'good_for_dinner', 'takes_reservations', 'outdoor_seating',\n",
    "                      'restaurant_is_expensive', 'has_alcohol', 'has_table_service', 'ambience_is_classy',\n",
    "                      'good_for_kids'])\n",
    "\n",
    "# make sure we have the caffenet weight downloaded.\n",
    "if not os.path.isfile(caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'):\n",
    "    print(\"Downloading pre-trained CaffeNet model...\")\n",
    "    !../scripts/download_model_binary.py ../models/bvlc_reference_caffenet\n",
    "\n",
    "# initialize caffe for gpu mode\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define network prototxts\n",
    "\n",
    "* Let's start by defining the nets using caffe.NetSpec. Note how we used the SigmoidCrossEntropyLoss layer. This is the right loss for multilabel classification. Also note how the data layer is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for common structures\n",
    "def conv_relu(bottom, ks, nout, stride=1, pad=0, group=1):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "                                num_output=nout, pad=pad, group=group)\n",
    "    return conv, L.ReLU(conv, in_place=True)\n",
    "\n",
    "# another helper function\n",
    "def fc_relu(bottom, nout):\n",
    "    fc = L.InnerProduct(bottom, num_output=nout)\n",
    "    return fc, L.ReLU(fc, in_place=True)\n",
    "\n",
    "# yet another helper function\n",
    "def max_pool(bottom, ks, stride=1):\n",
    "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n",
    "\n",
    "# main netspec wrapper\n",
    "def caffenet_multilabel(data_layer_params, datalayer):\n",
    "    # setup the python data layer \n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.label = L.Python(module = 'pascal_multilabel_datalayers', layer = datalayer, \n",
    "                               ntop = 2, param_str=str(data_layer_params))\n",
    "\n",
    "    # the net itself\n",
    "    n.conv1, n.relu1 = conv_relu(n.data, 11, 96, stride=4)\n",
    "    n.pool1 = max_pool(n.relu1, 3, stride=2)\n",
    "    n.norm1 = L.LRN(n.pool1, local_size=5, alpha=1e-4, beta=0.75)\n",
    "    n.conv2, n.relu2 = conv_relu(n.norm1, 5, 256, pad=2, group=2)\n",
    "    n.pool2 = max_pool(n.relu2, 3, stride=2)\n",
    "    n.norm2 = L.LRN(n.pool2, local_size=5, alpha=1e-4, beta=0.75)\n",
    "    n.conv3, n.relu3 = conv_relu(n.norm2, 3, 384, pad=1)\n",
    "    n.conv4, n.relu4 = conv_relu(n.relu3, 3, 384, pad=1, group=2)\n",
    "    n.conv5, n.relu5 = conv_relu(n.relu4, 3, 256, pad=1, group=2)\n",
    "    n.pool5 = max_pool(n.relu5, 3, stride=2)\n",
    "    n.fc6, n.relu6 = fc_relu(n.pool5, 4096)\n",
    "    n.drop6 = L.Dropout(n.relu6, in_place=True)\n",
    "    n.fc7, n.relu7 = fc_relu(n.drop6, 4096)\n",
    "    n.drop7 = L.Dropout(n.relu7, in_place=True)\n",
    "    n.score = L.InnerProduct(n.drop7, num_output=20)\n",
    "    n.loss = L.SigmoidCrossEntropyLoss(n.score, n.label)\n",
    "    \n",
    "    return str(n.to_proto())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write nets and solver files\n",
    "\n",
    "* Now we can crete net and solver prototxts. For the solver, we use the CaffeSolver class from the \"tools\" module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workdir = './yelp_multilabel'\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)\n",
    "\n",
    "solverprototxt = tools.CaffeSolver(trainnet_prototxt_path = osp.join(workdir, \"trainnet.prototxt\"),\n",
    "                                   testnet_prototxt_path = osp.join(workdir, \"valnet.prototxt\"))\n",
    "solverprototxt.sp['display'] = \"1\"\n",
    "solverprototxt.sp['base_lr'] = \"0.0001\"\n",
    "solverprototxt.write(osp.join(workdir, 'solver.prototxt'))\n",
    "\n",
    "# write train net.\n",
    "with open(osp.join(workdir, 'trainnet.prototxt'), 'w') as f:\n",
    "    # provide parameters to the data layer as a python dictionary. Easy as pie!\n",
    "    data_layer_params = dict(batch_size = 128, im_shape = [227, 227], split = 'train', yelp_root = yelp_root)\n",
    "    f.write(caffenet_multilabel(data_layer_params, 'PascalMultilabelDataLayerSync'))\n",
    "\n",
    "# write validation net.\n",
    "with open(osp.join(workdir, 'valnet.prototxt'), 'w') as f:\n",
    "    data_layer_params = dict(batch_size = 128, im_shape = [227, 227], split = 'val', yelp_root = yelp_root)\n",
    "    f.write(caffenet_multilabel(data_layer_params, 'PascalMultilabelDataLayerSync'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solver = caffe.SGDSolver(osp.join(workdir, 'solver.prototxt'))\n",
    "solver.net.copy_from(caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel')\n",
    "solver.test_nets[0].share_with(solver.net)\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's check the data we have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a766b04c082a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This is simply to add back the bias, re-shuffle the color channels to RGB, and so on...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m# First image in the batch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgtlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "transformer = tools.SimpleTransformer() # This is simply to add back the bias, re-shuffle the color channels to RGB, and so on...\n",
    "image_index = 0 # First image in the batch.\n",
    "plt.figure()\n",
    "plt.imshow(transformer.deprocess(copy(solver.net.blobs['data'].data[image_index, ...])))\n",
    "gtlist = solver.net.blobs['label'].data[image_index, ...].astype(np.int)\n",
    "plt.title('GT: {}'.format(classes[np.where(gtlist)]))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NOTE: we are readin the image from the data layer, so the resolution is lower than the original PASCAL image.\n",
    "\n",
    "### 4. Train a net.\n",
    "\n",
    "* Let's train the net. First, though, we need some way to measure the accuracy. Hamming distance is commonly used in multilabel problems. We also need a simple test loop. Let's write that down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hamming_distance(gt, est):\n",
    "    return sum([1 for (g, e) in zip(gt, est) if g == e]) / float(len(gt))\n",
    "\n",
    "def check_accuracy(net, num_batches, batch_size = 128):\n",
    "    acc = 0.0\n",
    "    for t in range(num_batches):\n",
    "        net.forward()\n",
    "        gts = net.blobs['label'].data\n",
    "        ests = net.blobs['score'].data > 0\n",
    "        for gt, est in zip(gts, ests): #for each ground truth and estimated label vector\n",
    "            acc += hamming_distance(gt, est)\n",
    "    return acc / (num_batches * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'solver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9be5e89cfc7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Now let it train for a while.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'itt:{:3d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy:{0:.4f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_nets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'solver' is not defined"
     ]
    }
   ],
   "source": [
    "# Now let it train for a while.\n",
    "for itt in range(6):\n",
    "    solver.step(100)\n",
    "    print 'itt:{:3d}'.format((itt + 1) * 100), 'accuracy:{0:.4f}'.format(check_accuracy(solver.test_nets[0], 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the accuracy is improving. And check for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_baseline_accuracy(net, num_batches, batch_size = 128):\n",
    "    acc = 0.0\n",
    "    for t in range(num_batches):\n",
    "        net.forward()\n",
    "        gts = net.blobs['label'].data\n",
    "        ests = np.zeros((batch_size, len(gts)))\n",
    "        for gt, est in zip(gts, ests): #for each ground truth and estimated label vector\n",
    "            acc += hamming_distance(gt, est)\n",
    "    return acc / (num_batches * batch_size)\n",
    "\n",
    "print 'Baseline accuracy:{0:.4f}'.format(check_baseline_accuracy(solver.test_nets[0], 5823/128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Look at some prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_net = solver.test_nets[0]\n",
    "for image_index in range(5):\n",
    "    plt.figure()\n",
    "    plt.imshow(transformer.deprocess(copy(test_net.blobs['data'].data[image_index, ...])))\n",
    "    gtlist = test_net.blobs['label'].data[image_index, ...].astype(np.int)\n",
    "    estlist = test_net.blobs['score'].data[image_index, ...] > 0\n",
    "    plt.title('GT: {} \\n EST: {}'.format(classes[np.where(gtlist)], classes[np.where(estlist)]))\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
